\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{booktabs}
\usepackage{float}
\usepackage{geometry}
\usepackage{multicol}
\geometry{margin=1in}

% Color definitions
\definecolor{zenblue}{RGB}{41,121,255}
\definecolor{zengreen}{RGB}{52,199,89}
\definecolor{zenorange}{RGB}{255,149,0}
\definecolor{zenpurple}{RGB}{175,82,222}

\hypersetup{
    colorlinks=true,
    linkcolor=zenblue,
    urlcolor=zenblue,
    citecolor=zenblue
}

\title{
    \vspace{-2cm}
    \Huge \textbf{The Zen AI Model Family} \\
    \vspace{0.5cm}
    \Large Democratizing AI Through Efficient Architecture \\
    \vspace{0.3cm}
    \normalsize Technical Overview and Architecture Whitepaper v1.0
}

\author{
    Hanzo AI Research Team \\
    \texttt{research@hanzo.ai} \\
    \\
    Zoo Labs Foundation \\
    \texttt{foundation@zoolabs.org}
}

\date{September 2025}

\begin{document}

\maketitle

\begin{abstract}
We introduce the \textbf{Zen AI Model Family}, a comprehensive suite of 10 state-of-the-art models spanning language understanding, 
visual creation, design analysis, and speech recognition. Built on cutting-edge Zen architectures and optimized
for efficiency, the Zen models achieve performance comparable to models 10x their size while reducing energy consumption by up to 98\%. 
This whitepaper presents the complete ecosystem including 5 language models (0.6B to 480B parameters), 2 artist models for image 
generation and editing, 2 designer models for visual reasoning, and 1 scribe model for speech recognition. Through innovative 
techniques including Mixture of Experts, extended thinking modes, and aggressive quantization, the Zen family democratizes 
access to frontier AI capabilities across diverse hardware platforms from edge devices to cloud infrastructure.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

The exponential growth in AI model capabilities has been accompanied by an equally dramatic increase in computational requirements, 
creating significant barriers to adoption and raising environmental concerns. The Zen AI Model Family addresses these challenges 
through a principled approach to model design that prioritizes efficiency without compromising capability.

\subsection{Mission and Vision}

Our mission is to democratize access to state-of-the-art AI capabilities through models that are:
\begin{itemize}
    \item \textbf{Efficient}: Optimized for minimal resource consumption
    \item \textbf{Capable}: Matching or exceeding larger models in key metrics
    \item \textbf{Accessible}: Deployable across diverse hardware platforms
    \item \textbf{Sustainable}: Designed with environmental impact in mind
    \item \textbf{Private}: Supporting on-device and private cloud deployment
\end{itemize}

\subsection{Key Innovations}

The Zen family introduces several architectural and training innovations:
\begin{enumerate}
    \item \textbf{Adaptive Parameter Activation}: MoE architectures that activate only necessary parameters
    \item \textbf{Extended Thinking Mode}: Up to 2M tokens for internal reasoning
    \item \textbf{Cross-Modal Synergy}: Unified architectures for multimodal understanding
    \item \textbf{Extreme Quantization}: 4-bit inference without significant quality loss
    \item \textbf{Hardware-Aware Design}: Optimizations for specific deployment targets
\end{enumerate}

\section{Model Family Overview}

\subsection{Complete Model Lineup}

The Zen family comprises 10 models across 4 categories:

\begin{table}[H]
\centering
\small
\begin{tabular}{llrrll}
\toprule
\textbf{Category} & \textbf{Model} & \textbf{Total} & \textbf{Active} & \textbf{Base} & \textbf{Focus} \\
\midrule
\multirow{5}{*}{Language} 
    & Zen-Nano & 0.6B & 0.6B & zen-0.5B & Mobile/IoT \\
    & Zen-Eco & 4B & 4B & zen-3B & Consumer \\
    & Zen-Omni & 30B & 30B & zen-32B & Multimodal \\
    & Zen-Coder & 480B & 30B & zen-Coder-32B & Code \\
    & Zen-Next & 80B & 80B & zen-72B & Flagship \\
\midrule
\multirow{2}{*}{Artist} 
    & Zen-Artist & 8B & 8B & Zen Artist architecture & Generation \\
    & Zen-Artist-Edit & 7B & 7B & Zen Artist architecture & Editing \\
\midrule
\multirow{2}{*}{Designer} 
    & Zen-Designer-Think & 235B & 22B & Zen MoDE-235B & Reasoning \\
    & Zen-Designer-Inst & 235B & 22B & Zen MoDE-235B & Generation \\
\midrule
Scribe & Zen-Scribe & 1.5B & 1.5B & Zen Scribe architecture & ASR \\
\bottomrule
\end{tabular}
\caption{Complete Zen Model Family Specifications}
\end{table}

\subsection{Capability Matrix}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{l|ccccc|cc|cc|c}
\toprule
\textbf{Capability} & \multicolumn{5}{c|}{\textbf{Language}} & \multicolumn{2}{c|}{\textbf{Artist}} & \multicolumn{2}{c|}{\textbf{Designer}} & \textbf{Scribe} \\
& Nano & Eco & Omni & Coder & Next & Artist & Edit & Think & Inst & ASR \\
\midrule
Text Generation & ✓ & ✓ & ✓ & ✓ & ✓ & × & × & ✓ & ✓ & × \\
Code Generation & ★ & ★★ & ★★★ & ★★★★★ & ★★★★ & × & × & ★★★ & ★★★ & × \\
Image Generation & × & × & × & × & × & ✓ & × & × & × & × \\
Image Editing & × & × & × & × & × & × & ✓ & × & × & × \\
Image Understanding & × & × & ✓ & × & × & ✓ & ✓ & ✓ & ✓ & × \\
Design Analysis & × & × & × & × & × & ★★ & ★★ & ★★★★★ & ★★★★★ & × \\
Speech Recognition & × & × & × & × & × & × & × & × & × & ✓ \\
Thinking Mode & ✓ & ✓ & ✓ & ✓ & ✓ & × & × & ✓ & × & × \\
\bottomrule
\end{tabular}
\caption{Model Capability Matrix (✓ = Supported, × = Not Supported, ★ = Capability Level)}
\end{table}

\section{Technical Architecture}

\subsection{Language Models}

\subsubsection{Zen-Nano (0.6B)}
Optimized for edge deployment, Zen-Nano achieves remarkable performance in just 0.6B parameters:
\begin{itemize}
    \item \textbf{Architecture}: Dense transformer with grouped-query attention
    \item \textbf{Context}: 32K tokens with 64K thinking tokens
    \item \textbf{Optimization}: INT4 quantization for 2GB memory footprint
    \item \textbf{Performance}: 51.7\% MMLU, 450 tokens/sec on edge devices
\end{itemize}

\subsubsection{Zen-Eco (4B)}
Balanced for consumer hardware:
\begin{itemize}
    \item \textbf{Architecture}: Enhanced transformer with Flash Attention v2
    \item \textbf{Context}: 32K tokens with 128K thinking tokens
    \item \textbf{Optimization}: Supports FP16, INT8, and INT4 deployment
    \item \textbf{Performance}: 62.3\% MMLU, runs on 8GB consumer GPUs
\end{itemize}

\subsubsection{Zen-Omni (30B)}
Multimodal text understanding:
\begin{itemize}
    \item \textbf{Architecture}: Unified transformer with cross-modal attention
    \item \textbf{Context}: 128K tokens with 256K thinking tokens
    \item \textbf{Optimization}: Efficient KV-cache management
    \item \textbf{Performance}: 68.4\% MMLU, native multimodal support
\end{itemize}

\subsubsection{Zen-Coder (480B MoE, 30B Active)}
Specialized for code generation:
\begin{itemize}
    \item \textbf{Architecture}: Mixture of 16 experts, 2 active
    \item \textbf{Context}: 128K tokens with 512K thinking tokens
    \item \textbf{Optimization}: Expert routing for code patterns
    \item \textbf{Performance}: 72.8\% HumanEval, syntax-aware generation
\end{itemize}

\subsubsection{Zen-Next (80B)}
Flagship model for maximum capability:
\begin{itemize}
    \item \textbf{Architecture}: Dense transformer with advanced attention
    \item \textbf{Context}: 128K tokens with 1M thinking tokens
    \item \textbf{Optimization}: Tensor parallelism for multi-GPU
    \item \textbf{Performance}: 75.6\% MMLU, state-of-the-art reasoning
\end{itemize}

\subsection{Artist Models}

\subsubsection{Zen-Artist (8B)}
Text-to-image generation:
\begin{itemize}
    \item \textbf{Architecture}: Diffusion-based generative model
    \item \textbf{Resolution}: Up to 1024x1024 native generation
    \item \textbf{Features}: Style control, prompt adherence, safety filters
    \item \textbf{Performance}: 88.5\% VQA accuracy, 50-step generation
\end{itemize}

\subsubsection{Zen-Artist-Edit (7B)}
Image editing and inpainting:
\begin{itemize}
    \item \textbf{Architecture}: Encoder-decoder with attention injection
    \item \textbf{Capabilities}: Object removal, style transfer, inpainting
    \item \textbf{Features}: Mask-based editing, semantic understanding
    \item \textbf{Performance}: 91.2\% VQA accuracy, real-time editing
\end{itemize}

\subsection{Designer Models}

\subsubsection{Zen-Designer-Thinking (235B MoE, 22B Active)}
Visual reasoning and analysis:
\begin{itemize}
    \item \textbf{Architecture}: Vision-language MoE with 2M thinking tokens
    \item \textbf{Context}: 131K multimodal tokens
    \item \textbf{Capabilities}: Design critique, accessibility analysis, layout optimization
    \item \textbf{Performance}: 96.3\% VQA accuracy, 94.2\% DesignBench
\end{itemize}

\subsubsection{Zen-Designer-Instruct (235B MoE, 22B Active)}
Design generation and modification:
\begin{itemize}
    \item \textbf{Architecture}: Vision-language MoE optimized for generation
    \item \textbf{Context}: 131K multimodal tokens with 512K thinking
    \item \textbf{Capabilities}: UI/UX generation, design system creation
    \item \textbf{Performance}: 95.8\% VQA accuracy, 92.1\% DesignBench
\end{itemize}

\subsection{Scribe Model}

\subsubsection{Zen-Scribe (1.5B)}
Speech recognition and transcription:
\begin{itemize}
    \item \textbf{Architecture}: Encoder-decoder with CTC/attention hybrid
    \item \textbf{Languages}: 98 languages with accent robustness
    \item \textbf{Features}: Real-time streaming, speaker diarization
    \item \textbf{Performance}: 3.2\% WER on diverse datasets
\end{itemize}

\section{Training Methodology}

\subsection{Data Curation}

Our training pipeline emphasizes quality over quantity:
\begin{enumerate}
    \item \textbf{Web-scale corpus}: 7T tokens filtered for quality
    \item \textbf{Domain-specific data}: Code, scientific papers, creative writing
    \item \textbf{Multimodal pairs}: 500M image-text pairs, 100M audio samples
    \item \textbf{Synthetic generation}: Targeted data for edge cases
    \item \textbf{Human feedback}: 10M preference comparisons
\end{enumerate}

\subsection{Training Process}

\begin{figure}[H]
\centering
\begin{verbatim}
    [Pretraining] → [SFT] → [RLHF] → [Constitutional AI] → [Deployment]
         ↓            ↓        ↓              ↓                  ↓
    Base Model   Task Tuning  Alignment  Safety Filters   Quantization
\end{verbatim}
\caption{Zen Model Training Pipeline}
\end{figure}

\subsection{Efficiency Optimizations}

Key techniques for reducing training and inference costs:
\begin{itemize}
    \item \textbf{Mixed Precision Training}: FP16/BF16 with FP32 accumulation
    \item \textbf{Gradient Checkpointing}: 40\% memory reduction
    \item \textbf{Flash Attention}: 3x speedup in attention computation
    \item \textbf{Q-GaLore Fine-tuning}: Quantized gradient low-rank projection reduces optimizer memory by up to 50\% vs.\ LoRA while outperforming QLoRA by up to 5.19 MMLU points at equivalent memory budgets
    \item \textbf{Knowledge Distillation}: Transfer from larger teachers via Zen MoDE (Mixture of Distilled Experts)
\end{itemize}

\subsection{Continual Learning and Adaptation}

Production AI systems require continual adaptation without catastrophic forgetting of prior capabilities. The Zen family incorporates a layered continual learning strategy based on the current empirical consensus~\cite{wang2025survey, lifelong2025survey}:

\begin{enumerate}
    \item \textbf{Base Capability Preservation (OPLoRA)}: All fine-tuning constrains LoRA updates to be orthogonal to the top singular vectors of frozen base weight matrices~\cite{oplora2025}, protecting the most information-rich directions of the pretrained model from overwriting.

    \item \textbf{Surprise-Driven Replay (SuRe)}: Our continual fine-tuning pipeline uses NLL-prioritized experience replay --- sequences where the model exhibits high negative log-likelihood (high surprise) are preferentially retained in the replay buffer, ensuring the most at-risk knowledge is rehearsed~\cite{sure2025}. A dual-learner design with fast LoRA (rapid adaptation) and slow LoRA (EMA-stabilized) is merged after each training phase.

    \item \textbf{Sequential Model Merging (OPCM)}: As specialized Zen variants are released (Zen-Coder, Zen-Eco, Zen-Omni, etc.), we apply Orthogonal Projection-based Continual Merging~\cite{opcm2025} to sequentially integrate task-specific capabilities into a unified base checkpoint. OPCM maintains $O(|\theta|)$ memory complexity regardless of the number of merged models and achieves 5--8\% average accuracy improvement over simultaneous merging approaches.

    \item \textbf{Training-Free Agent Adaptation}: For deployment-time adaptation without weight updates, we employ in-context experience accumulation inspired by the Youtu-Agent framework~\cite{youtu2025}, enabling GRPO-equivalent policy improvement (+2.7\% AIME 2024) purely through structured context management.
\end{enumerate}

\subsection{Multi-Variant Deployment via Delta Compression}

The Zen family serves multiple fine-tuned variants (instruct, coder, abliterated) from shared base weights using BitDelta~\cite{bitdelta2024}:

$$W_{\text{ft}} = W_{\text{base}} + \hat{\Delta}, \quad \hat{\Delta} = \alpha \cdot \text{sign}(\Delta)$$

where $\alpha$ is a per-matrix scalar learned via distillation on a calibration set. This achieves greater than 10$\times$ memory reduction for variant-specific deltas, enabling multi-tenant serving of all Zen variants from a single high-precision base model in shared GPU memory.

For edge and on-device deployment, K-Merge~\cite{kmerge2025} manages LoRA adapter updates continuously within a fixed storage budget, merging newly received adapters into the closest existing stored adapter via task-vector cosine similarity.

\section{Performance Benchmarks}

\subsection{Language Understanding}

\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{MMLU} & \textbf{HumanEval} & \textbf{GSM8K} & \textbf{HellaSwag} & \textbf{ARC} & \textbf{Avg} \\
\midrule
Zen-Nano & 51.7 & 22.6 & 62.0 & 59.5 & 48.3 & 48.8 \\
Zen-Eco & 62.3 & 35.2 & 74.8 & 71.6 & 59.7 & 60.7 \\
Zen-Omni & 68.4 & 48.3 & 82.1 & 78.7 & 66.2 & 68.7 \\
Zen-Coder & 78.9 & 72.8 & 94.7 & 90.8 & 76.5 & 82.7 \\
Zen-Next & 75.6 & 61.7 & 90.7 & 87.0 & 73.1 & 77.6 \\
\bottomrule
\end{tabular}
\caption{Language Model Benchmark Results (\%)}
\end{table}

\subsection{Visual Understanding}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{VQA v2} & \textbf{DesignBench} & \textbf{CLIP Score} & \textbf{FID} \\
\midrule
Zen-Artist & 88.5 & 82.4 & 84.1 & 23.5 \\
Zen-Artist-Edit & 91.2 & 87.3 & 86.6 & 18.7 \\
Zen-Designer-Think & 96.3 & 94.2 & 91.5 & - \\
Zen-Designer-Inst & 95.8 & 92.1 & 91.0 & - \\
\bottomrule
\end{tabular}
\caption{Visual Model Benchmark Results}
\end{table}

\subsection{Speech Recognition}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{WER (\%)} & \textbf{Languages} & \textbf{RTF} & \textbf{Accuracy} \\
\midrule
LibriSpeech (clean) & 2.8 & English & 0.15 & 97.2 \\
Common Voice & 4.1 & 98 & 0.18 & 95.9 \\
Multilingual ASR & 5.2 & 98 & 0.20 & 94.8 \\
\bottomrule
\end{tabular}
\caption{Zen-Scribe ASR Performance (RTF = Real-Time Factor)}
\end{table}

\section{Deployment and Integration}

\subsection{Deployment Options}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccccl}
\toprule
\textbf{Format} & \textbf{Precision} & \textbf{Size} & \textbf{Speed} & \textbf{Quality} & \textbf{Platform} \\
\midrule
SafeTensors & FP16 & 100\% & Baseline & 100\% & All \\
GGUF & Q8\_0 & 50\% & 1.8x & 99.5\% & CPU/GPU \\
GGUF & Q5\_K\_M & 31\% & 2.2x & 99.2\% & CPU/GPU \\
GGUF & IQ4\_XS & 24\% & 2.6x & 99.0\% & CPU/GPU \\
GGUF & Q4\_K\_M & 25\% & 2.5x & 98.5\% & CPU/GPU \\
GGUF & IQ3\_M & 18\% & 3.0x & 96.8\% & CPU/Edge \\
GGUF & IQ2\_M & 13\% & 3.5x & 91.2\% & Extreme edge \\
MLX & 4-bit & 25\% & 3x & 98\% & Apple Silicon \\
ONNX & INT8 & 50\% & 2x & 99\% & Cross-platform \\
\bottomrule
\end{tabular}
\caption{Deployment Format Comparison. IQ (Importance-Quantization) formats use non-linear table-assisted reconstruction with an importance matrix~\cite{gguf2024} and consistently outperform K-quants at equivalent bit widths. IQ4\_XS exceeds Q4\_K\_M quality at similar size.}
\end{table}

\subsection{Hardware Requirements}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{lrrrrr}
\toprule
\textbf{Model} & \textbf{FP16} & \textbf{INT8} & \textbf{INT4} & \textbf{Min Device} & \textbf{Recommended} \\
\midrule
Zen-Nano & 1.2GB & 0.6GB & 0.3GB & RPi 4 (2GB) & RPi 5 (8GB) \\
Zen-Eco & 8GB & 4GB & 2GB & Laptop (8GB) & M2 MacBook \\
Zen-Artist & 16GB & 8GB & 4GB & RTX 3060 & RTX 3080 \\
Zen-Omni & 60GB & 30GB & 15GB & RTX 4090 & A100 40GB \\
Zen-Coder & 240GB & 120GB & 60GB & A100 80GB & 2x A100 \\
Zen-Next & 160GB & 80GB & 40GB & 2x RTX 4090 & 2x A100 \\
Zen-Designer & 220GB & 110GB & 55GB & A100 80GB & 2x A100 \\
Zen-Scribe & 3GB & 1.5GB & 0.8GB & Phone (4GB) & Any GPU \\
\bottomrule
\end{tabular}
\caption{Memory Requirements by Precision}
\end{table}

\subsection{Integration Examples}

\subsubsection{Python Integration}
\begin{lstlisting}[language=Python]
# Unified interface for all Zen models
from zen import AutoModel, AutoProcessor

# Load any Zen model
model = AutoModel.from_pretrained("zenlm/zen-eco-4b-instruct")
processor = AutoProcessor.from_pretrained("zenlm/zen-eco-4b-instruct")

# Enable thinking mode for supported models
response = model.generate(
    "Solve this complex problem",
    max_thinking_tokens=100000,
    max_response_tokens=2000
)
\end{lstlisting}

\subsubsection{REST API}
\begin{lstlisting}[language=bash]
# Deploy with Docker
docker run -p 8080:8080 zenlm/zen-api:latest \
  --model zen-eco-4b-instruct \
  --quantization int4

# Query the API
curl -X POST http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello, world!", "max_tokens": 100}'
\end{lstlisting}

\section{Environmental Impact}

\subsection{Sustainability Metrics}

The Zen family achieves unprecedented efficiency:

\begin{table}[H]
\centering
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Energy/Token} & \textbf{CO₂/M Inferences} & \textbf{Efficiency Gain} \\
\midrule
Zen-Nano & 0.001 kWh & 0.02 kg & 98\% \\
Zen-Eco & 0.003 kWh & 0.05 kg & 95\% \\
Zen-Omni & 0.015 kWh & 0.25 kg & 85\% \\
Zen-Coder & 0.008 kWh & 0.40 kg & 92\% \\
Zen-Next & 0.025 kWh & 0.45 kg & 80\% \\
All Models (Avg) & 0.010 kWh & 0.23 kg & 90\% \\
\bottomrule
\end{tabular}
\caption{Environmental Impact Metrics}
\end{table}

\subsection{Annual Impact (1M Users)}
\begin{itemize}
    \item \textbf{Energy Saved}: 45 GWh (equivalent to 10,000 homes)
    \item \textbf{CO₂ Reduced}: 5,400 tons (equivalent to 1,200 cars)
    \item \textbf{Cost Savings}: \$2.7M in compute costs
    \item \textbf{Water Conservation}: 2.3M gallons saved in cooling
\end{itemize}

\section{Safety and Alignment}

\subsection{Safety Measures}

Comprehensive safety framework:
\begin{enumerate}
    \item \textbf{Constitutional AI}: Trained with harmlessness constraints
    \item \textbf{Red Teaming}: 500+ hours of adversarial testing
    \item \textbf{Content Filtering}: Multi-layer safety classifiers
    \item \textbf{Uncertainty Quantification}: Confidence-aware responses
    \item \textbf{Audit Trail}: Complete inference logging capability
\end{enumerate}

\subsection{Ethical Considerations}

\begin{itemize}
    \item \textbf{Bias Mitigation}: Diverse training data, regular audits
    \item \textbf{Privacy}: On-device deployment, no data collection
    \item \textbf{Transparency}: Open model cards, clear limitations
    \item \textbf{Accessibility}: Models for low-resource environments
    \item \textbf{Sustainability}: Carbon-neutral training commitment
\end{itemize}

\section{Future Directions}

\subsection{Roadmap}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Timeline} & \textbf{Milestone} \\
\midrule
Q4 2025 & Extended context to 1M tokens \\
Q1 2026 & Real-time video understanding \\
Q2 2026 & Unified multimodal architecture \\
Q3 2026 & Edge deployment optimization \\
Q4 2026 & Zen v2.0 with neural architecture search \\
\bottomrule
\end{tabular}
\caption{Development Roadmap}
\end{table}

\subsection{Research Priorities}

\begin{enumerate}
    \item \textbf{IQ-Quant Pipeline}: Deploy IQ2--IQ4 GGUF variants with importance matrices for all Zen models, enabling viable 2-bit edge deployment~\cite{gguf2024}
    \item \textbf{SuRe Continual Learning}: Implement surprise-driven prioritized replay with dual fast/slow LoRA EMA for all future Zen fine-tuning runs~\cite{sure2025}
    \item \textbf{Drop-Upcycling MoE Expansion}: Evaluate Drop-Upcycling~\cite{dropupcycling2025} for expanding dense Zen models to MoE architecture, achieving 13B-equivalent performance with 5.9B active parameters at 1/4 training FLOPs
    \item \textbf{GT-QLoRA for Ultra}: Complete Gate-Targeted QLoRA training for Zen4-Ultra to address MoE abliteration failure modes where refusal is encoded in expert routing rather than the residual stream
    \item \textbf{MonoSoup Checkpoint Selection}: Apply MonoSoup SVD decomposition~\cite{monosoup2026} during training to recover up to 8\% on weaker checkpoints without additional fine-tuning runs
    \item \textbf{Streaming Agent Adaptation}: Integrate stateful replay~\cite{statefulreplay2025} for models that must update continuously from live interaction streams
\end{enumerate}

\section{Conclusion}

The Zen AI Model Family represents a paradigm shift in AI development, proving that exceptional capability and 
efficiency are not mutually exclusive. Through innovative architectures, training techniques, and deployment 
strategies, we have created a comprehensive ecosystem of models that democratize access to frontier AI while 
reducing environmental impact by up to 98\%.

With 10 models spanning language, vision, design, and speech, the Zen family provides solutions for every use 
case from edge IoT devices to enterprise deployments. Our commitment to open science, sustainability, and 
responsible AI ensures that the benefits of artificial intelligence are accessible to all while preserving 
our planet for future generations.

\section*{Acknowledgments}

We thank the open-source community, particularly the teams behind Transformers, GGML, and the broader AI research ecosystem. Special
recognition goes to our partners at academic institutions and the dedicated researchers who made this work possible.

\appendix

\section{Model Availability}

All Zen models are available at:
\begin{itemize}
    \item \textbf{HuggingFace}: \url{https://huggingface.co/zenlm}
    \item \textbf{GitHub}: \url{https://github.com/zenlm/zen}
    \item \textbf{Documentation}: \url{https://docs.hanzo.ai/zen}
\end{itemize}

\section{Citation}

\begin{verbatim}
@article{zen2025,
  title={The Zen AI Model Family: Democratizing AI Through Efficient Architecture},
  author={Hanzo AI Research and Zoo Labs Foundation},
  journal={arXiv preprint},
  year={2025}
}
\end{verbatim}

\begin{thebibliography}{99}

\bibitem{bitdelta2024}
Liu, J., Xiao, G., Li, K., Lee, J.D., Han, S., Dao, T., \& Cai, T. (2024).
\textit{BitDelta: Your Fine-Tune May Only Be Worth One Bit}.
NeurIPS 2024. arXiv:2402.10193.

\bibitem{sure2025}
SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning (2025).
arXiv:2511.22367.

\bibitem{opcm2025}
Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging (2025).
arXiv:2501.09522.

\bibitem{oplora2025}
OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning (2025).
arXiv:2510.13003.

\bibitem{dropupcycling2025}
Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization (2025).
arXiv:2502.19261.

\bibitem{monosoup2026}
Model soups need only one ingredient (2026).
arXiv:2602.09689.

\bibitem{kmerge2025}
K-Merge: Online Continual Merging of Adapters for On-device Large Language Models (2025).
arXiv:2510.13537.

\bibitem{youtu2025}
Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization (2025).
arXiv:2512.24615.

\bibitem{wang2025survey}
Wang et al. Continual Learning of Large Language Models: A Comprehensive Survey.
ACM Computing Surveys 2025. arXiv:2404.16789.

\bibitem{lifelong2025survey}
Towards Lifelong Learning of Large Language Models: A Survey.
ACM CSUR 2025. DOI:10.1145/3716629.

\bibitem{gguf2024}
IQ Quantization Methods in llama.cpp: Non-linear table-assisted reconstruction with importance matrices.
ggml-org/llama.cpp, 2024--2025.

\end{thebibliography}

\end{document}