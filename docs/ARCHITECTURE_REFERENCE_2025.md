# Zen AI Architecture Reference Guide
## Updated: September 25, 2025

This document provides comprehensive technical specifications for all Zen AI models using the latest Qwen3 architectures. This reference is essential for developers working with our models and should be included in all training datasets.

---

## üèóÔ∏è Architecture Overview

As of September 25, 2025, the Zen AI ecosystem consists of five primary architectures:

| Model | Base Architecture | Type | Total Params | Active Params | Context | Release |
|-------|------------------|------|--------------|---------------|---------|---------|
| **Zen-Nano** | Qwen3-0.6B | Dense | 600M | 600M | 32K | Sept 2025 |
| **Zen-Eco** | Qwen3-4B | Dense | 4B | 4B | 32K | Sept 2025 |
| **Zen-Coder** | Qwen3-Coder-480B-A35B | MoE | 480B | 35B | 128K | Sept 2025 |
| **Zen-Omni** | Qwen3-Omni-30B-A3B | MoE | 30B | 3B | 64K | Sept 2025 |
| **Zen-Next** | Qwen3-Next-80B-A3B | MoE | 80B | 3B | 128K | Sept 2025 |

---

## üìä Detailed Architecture Specifications

### Zen-Nano (Qwen3-0.6B)
**Ultra-Efficient Edge Model**

```yaml
Architecture: Dense Transformer
Parameters: 600,000,000
Layers: 24
Hidden Size: 1024
Attention Heads: 16
FFN Dimension: 2816
Vocab Size: 151,936
Max Position: 32,768
Activation: SwiGLU
Normalization: RMSNorm
Attention: Multi-Head Self-Attention
```

**Key Features:**
- Optimized for mobile and IoT devices
- Sub-second response times
- Runs on devices with 1GB RAM
- INT4 quantization: 300MB model size
- Supports 100+ tokens/second on mobile CPUs

**Use Cases:**
- Real-time chatbots
- Edge AI assistants
- IoT device intelligence
- Mobile applications
- Embedded systems

### Zen-Eco (Qwen3-4B)
**Balanced Performance Model**

```yaml
Architecture: Dense Transformer with GQA
Parameters: 4,000,000,000
Layers: 28
Hidden Size: 3584
Query Heads: 28
KV Heads: 4 (7:1 GQA ratio)
FFN Dimension: 9856
Vocab Size: 151,936
Max Position: 32,768
Activation: SwiGLU
Normalization: RMSNorm
Rope Theta: 1,000,000
```

**Optimizations:**
- Grouped Query Attention (75% memory reduction)
- Flash Attention 2 support
- Rotary Position Embeddings (RoPE)
- Sliding window attention (4K local, 32K global)

**Performance:**
- 45-52 tokens/sec on Apple M2
- 65-75 tokens/sec on RTX 4090
- 2GB memory with INT4 quantization
- 4GB memory with INT8 quantization

### Zen-Coder (Qwen3-Coder-480B-A35B)
**Massive MoE for Code Generation**

```yaml
Architecture: Mixture of Experts (MoE)
Total Parameters: 480,000,000,000
Active Parameters: 35,000,000,000
Number of Experts: 64
Experts per Token: 8
Layers: 80
Hidden Size: 8192
Query Heads: 64
KV Heads: 8 (8:1 GQA ratio)
Expert FFN: 28,672
Shared FFN: 4,096
Vocab Size: 161,000 (code-optimized)
Max Position: 128,000
Router Type: Top-K with auxiliary loss
Load Balancing: Yes (coefficient 0.01)
```

**Expert Specialization:**
- 8 experts: Python/Data Science
- 8 experts: JavaScript/TypeScript/Web
- 8 experts: Systems (C/C++/Rust/Go)
- 8 experts: Enterprise (Java/C#/.NET)
- 8 experts: Mobile (Swift/Kotlin/Flutter)
- 8 experts: DevOps/Infrastructure
- 8 experts: Databases/SQL
- 8 experts: General purpose

**Code-Specific Features:**
- Fill-in-the-middle (FIM) capability
- Repository-level context understanding
- Multi-file editing support
- 150+ programming languages
- Syntax-aware tokenization

### Zen-Omni (Qwen3-Omni-30B-A3B)
**Multimodal MoE Model**

```yaml
Architecture: Multimodal Mixture of Experts
Total Parameters: 30,000,000,000
Active Parameters: 3,000,000,000
Number of Experts: 32
Experts per Token: 4
Text Backbone: 28B MoE
Vision Encoder: ViT-L/14 (300M)
Audio Encoder: Whisper-large-v3 (1.5B)
Cross-Modal Layers: Every 4th layer
Modality Tokens: <image>, <audio>, <video>
```

**Multimodal Capabilities:**
- **Vision**: 1024√ó1024 resolution, object detection, OCR
- **Audio**: 16kHz sampling, 30-second chunks, 100+ languages
- **Video**: 32 frames at 224√ó224, temporal understanding
- **Cross-Modal**: Unified representation learning

**Expert Allocation:**
- 8 experts: Vision processing
- 8 experts: Audio processing
- 8 experts: Text processing
- 8 experts: Cross-modal reasoning

### Zen-Next (Qwen3-Next-80B-A3B)
**Ultra-Sparse MoE for Maximum Efficiency**

```yaml
Architecture: Ultra-Sparse Mixture of Experts
Total Parameters: 80,000,000,000
Active Parameters: 3,000,000,000
Sparsity: 96.25%
Number of Experts: 128
Experts per Token: 2 (ultra-sparse)
Layers: 60
Hidden Size: 4096
Query Heads: 32
KV Heads: 4 (8:1 GQA ratio)
Expert FFN: 14,336
Max Position: 128,000
Router: Learned with temperature control
Dynamic Expert Allocation: Yes
```

**128 Expert Specialization Map:**
- 16 experts: Mathematical reasoning
- 16 experts: Scientific domains
- 16 experts: Programming (by language family)
- 16 experts: Natural languages
- 16 experts: Creative tasks
- 16 experts: Analytical reasoning
- 16 experts: Tool use & function calling
- 16 experts: Safety & alignment

**Ultra-Sparse Benefits:**
- 96.25% compute reduction
- Runs on single GPU (3B active)
- 60-80 tokens/second
- Matches GPT-4 performance

---

## üöÄ Deployment Configurations

### Memory Requirements

| Model | FP32 | FP16 | INT8 | INT4 | Production |
|-------|------|------|------|------|------------|
| Zen-Nano | 2.4GB | 1.2GB | 600MB | 300MB | 2-4GB RAM |
| Zen-Eco | 16GB | 8GB | 4GB | 2GB | 8-16GB RAM |
| Zen-Coder | 1.92TB/140GB | 960GB/70GB | 480GB/35GB | 240GB/17.5GB | 80GB VRAM |
| Zen-Omni | 120GB/12GB | 60GB/6GB | 30GB/3GB | 15GB/1.5GB | 8-16GB VRAM |
| Zen-Next | 320GB/12GB | 160GB/6GB | 80GB/3GB | 40GB/1.5GB | 8-16GB VRAM |

*Note: For MoE models, format is Total/Active*

### Inference Performance

| Model | Apple M2 | RTX 4090 | A100 | H100 | Mobile |
|-------|----------|----------|------|------|--------|
| Zen-Nano | 80-100 | 150-200 | 200+ | 300+ | 50-80 |
| Zen-Eco | 45-52 | 65-75 | 80-90 | 100+ | 15-20 |
| Zen-Coder | - | 25-30 | 35-40 | 45-50 | - |
| Zen-Omni | 30-40 | 40-50 | 50-60 | 70-80 | - |
| Zen-Next | 40-50 | 60-80 | 80-100 | 120+ | - |

*Performance in tokens/second*

---

## üîß Training Configurations

### LoRA Fine-tuning Parameters

| Model | Rank | Alpha | Target Modules | Trainable % |
|-------|------|-------|----------------|-------------|
| Zen-Nano | 8 | 16 | q,v,o,gate,up,down | 2.1% |
| Zen-Eco | 16 | 32 | q,v,o,gate,up,down | 1.2% |
| Zen-Coder | 32 | 64 | router,experts.*.wi,wo | 0.4% |
| Zen-Omni | 16 | 32 | cross_attn,experts.*.mlp | 0.8% |
| Zen-Next | 64 | 128 | sparse targeting | 0.3% |

### Training Data Requirements

| Model | Tokens | Batch Size | Learning Rate | Epochs |
|-------|--------|------------|---------------|--------|
| Zen-Nano | 100B | 64 | 5e-5 | 3-5 |
| Zen-Eco | 500B | 32 | 2e-5 | 3 |
| Zen-Coder | 3T | 4 | 1e-5 | 2 |
| Zen-Omni | 1T | 8 | 2e-6 | 3 |
| Zen-Next | 5T | 2 | 5e-7 | 2 |

---

## üõ†Ô∏è Implementation Examples

### Basic Inference

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load any Zen model
model = AutoModelForCausalLM.from_pretrained("zenlm/zen-eco")
tokenizer = AutoTokenizer.from_pretrained("zenlm/zen-eco")

# Generate
inputs = tokenizer("Hello, how are you?", return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
response = tokenizer.decode(outputs[0])
```

### MoE-Specific Configuration

```python
# For Zen-Coder, Zen-Omni, Zen-Next
model = AutoModelForCausalLM.from_pretrained(
    "zenlm/zen-coder",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    # MoE specific
    num_experts=64,
    experts_per_token=8,
    router_aux_loss_coef=0.01,
    load_balancing=True
)
```

### Multimodal Processing (Zen-Omni)

```python
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("zenlm/zen-omni")
model = AutoModelForVision2Seq.from_pretrained("zenlm/zen-omni")

# Process image and text
inputs = processor(
    text="What's in this image?",
    images=image,
    return_tensors="pt"
)
outputs = model.generate(**inputs)
```

---

## üìà Performance Benchmarks (September 2025)

| Model | MMLU | GSM8K | HumanEval | HellaSwag | Average |
|-------|------|-------|-----------|-----------|---------|
| Zen-Nano | 42.3% | 28.1% | 18.2% | 68.4% | 39.3% |
| Zen-Eco | 51.7% | 32.4% | 22.6% | 76.4% | 45.8% |
| Zen-Coder | 78.9% | 71.2% | 91.3% | 88.7% | 82.5% |
| Zen-Omni | 65.4% | 58.3% | 45.2% | 81.2% | 62.5% |
| Zen-Next | 87.3% | 92.1% | 84.6% | 95.2% | 89.8% |

---

## üåç Environmental Impact

| Model | Power (W) | CO‚ÇÇ/month (kg) | Energy/month (kWh) |
|-------|-----------|----------------|-------------------|
| Zen-Nano | 5 | 0.012 | 3.6 |
| Zen-Eco | 15 | 0.036 | 10.8 |
| Zen-Coder | 350 | 0.84 | 252 |
| Zen-Omni | 50 | 0.12 | 36 |
| Zen-Next | 50 | 0.12 | 36 |

*Based on continuous operation*

---

## üì¶ Available Formats

All models are available in multiple formats:

- **SafeTensors**: Default PyTorch format
- **GGUF**: For llama.cpp (Q4_K_M, Q5_K_M, Q8_0)
- **MLX**: Optimized for Apple Silicon
- **ONNX**: Cross-platform deployment
- **TensorRT**: NVIDIA GPU optimization
- **OpenVINO**: Intel hardware optimization

---

## üîó Resources

- **GitHub**: [github.com/zenlm](https://github.com/zenlm)
- **HuggingFace**: [huggingface.co/zenlm](https://huggingface.co/zenlm)
- **Documentation**: [docs.zenai.org](https://docs.zenai.org)
- **Zoo-gym Training**: [github.com/zooai/gym](https://github.com/zooai/gym)

---

## üìù Citation

```bibtex
@article{zen_architectures_2025,
  title={Zen AI: Efficient Language Models with Advanced Architectures},
  author={Hanzo AI Research and Zoo Labs Foundation},
  journal={Technical Report},
  year={2025},
  month={September},
  version={2025.09.25}
}
```

---

**Built with ‚ù§Ô∏è by Hanzo AI (Techstars '24) & Zoo Labs Foundation (501(c)(3))**

*Last Updated: September 25, 2025*